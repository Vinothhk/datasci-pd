{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Person   Age       City Bool  Marks\n",
      "0  Person2  24.0  Bangalore   No     47\n",
      "1  Person3  19.0      Delhi   No     89\n",
      "2  Person4   NaN     Mumbai  Yes     93\n",
      "3  Person5  24.0  Hyderabad   No     85\n",
      "4  Person1  17.0    Chennai  Yes     98\n"
     ]
    }
   ],
   "source": [
    "#Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "\n",
    "df  = pd.read_csv('datasets/dataset1.csv')     # Read the file\n",
    "print(df)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    24.0\n",
      "1    19.0\n",
      "2     NaN\n",
      "3    24.0\n",
      "4    17.0\n",
      "Name: Age, dtype: float64\n",
      "\n",
      "\n",
      "             Age      Marks\n",
      "count   4.000000   5.000000\n",
      "mean   21.000000  82.400000\n",
      "std     3.559026  20.366639\n",
      "min    17.000000  47.000000\n",
      "25%    18.500000  85.000000\n",
      "50%    21.500000  89.000000\n",
      "75%    24.000000  93.000000\n",
      "max    24.000000  98.000000\n",
      "\n",
      "\n",
      "(5, 5)\n",
      "\n",
      "\n",
      "   Person    Age   City   Bool  Marks\n",
      "0   False  False  False  False  False\n",
      "1   False  False  False  False  False\n",
      "2   False   True  False  False  False\n",
      "3   False  False  False  False  False\n",
      "4   False  False  False  False  False\n"
     ]
    }
   ],
   "source": [
    "print(df['Age'])    # Access Coloumn\n",
    "print('\\n')\n",
    "\n",
    "print(df.describe())  # Generate descriptive statistics\n",
    "print('\\n')\n",
    "\n",
    "print(df.shape)                     #representing the dimensionality of the DataFrame\n",
    "print('\\n')\n",
    "\n",
    "print(df.isnull())                  #Return a boolean same-sized object indicating if the values are NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Person   Age       City Bool  Marks\n",
      "0  Person2  24.0  Bangalore   No     47\n",
      "1  Person3  19.0      Delhi   No     89\n",
      "2  Person4   NaN     Mumbai  Yes     93\n",
      "3  Person5  24.0  Hyderabad   No     85\n",
      "4  Person1  17.0    Chennai  Yes     98\n",
      "\n",
      "\n",
      "    Person   Age       City Bool  Marks\n",
      "0  Person2  24.0  Bangalore   No     47\n",
      "1  Person3  19.0      Delhi   No     89\n",
      "2  Person4   NaN     Mumbai  Yes     93\n",
      "\n",
      "\n",
      "    Person   Age       City Bool  Marks\n",
      "0  Person2  24.0  Bangalore   No     47\n",
      "\n",
      "\n",
      "19.0\n"
     ]
    }
   ],
   "source": [
    "print(df); print('\\n')\n",
    "\n",
    "print(df.head(3))                       # Returns first n rows\n",
    "print('\\n')\n",
    "\n",
    "print(df.loc[df['City']=='Bangalore'])  # Access a group of rows and columns by label(s) \n",
    "print('\\n')\n",
    "\n",
    "print(df.iloc[1,1])                     # Purely integer-location based indexing for selection by position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.groupby - Group DataFrame using a mapper or by a Series of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Bool                 Person   Age                     City  Marks\n",
      "0   No  Person2Person3Person5  67.0  BangaloreDelhiHyderabad    221\n",
      "1  Yes         Person4Person1  17.0            MumbaiChennai    191\n",
      "\n",
      "\n",
      "Bool\n",
      "No     22.333333\n",
      "Yes    17.000000\n",
      "Name: Age, dtype: float64\n",
      "\n",
      "\n",
      "    Person   Age       City Bool  Marks\n",
      "0  Person2  24.0  Bangalore   No     47\n",
      "1  Person3  19.0      Delhi   No     89\n",
      "3  Person5  24.0  Hyderabad   No     85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_458387/2869077663.py:5: FutureWarning: The provided callable <function mean at 0x7584e063a3b0> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  print(x['Age'].agg(np.mean))\n",
      "/tmp/ipykernel_458387/2869077663.py:8: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n",
      "  print(x.get_group('No'))         # Print Rows of a specific group\n"
     ]
    }
   ],
   "source": [
    "x = df.groupby(['Bool'])         # Group DataFrame using a mapper or by a Series of columns.\n",
    "print(x.sum().reset_index())\n",
    "print('\\n')\n",
    "\n",
    "print(x['Age'].agg(np.mean))\n",
    "print('\\n')\n",
    "\n",
    "print(x.get_group('No'))         # Print Rows of a specific group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### describe()\n",
    "\n",
    "Generate descriptive statistics that include those that summarize the central\n",
    "tendency, dispersion and shape of a\n",
    "dataset's distribution, excluding ``NaN`` values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2 = pd.read_csv('datasets/loans.csv')                 #Reads loans.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Car       Model  Volume  Weight  CO2  Unnamed: 5\n",
      "0       Toyota        Aygo    1000     790   99         NaN\n",
      "1   Mitsubishi  Space Star    1200    1160   95         NaN\n",
      "2        Skoda      Citigo    1000     929   95         NaN\n",
      "3         Fiat         500     900     865   90         NaN\n",
      "4         Mini      Cooper    1500    1140  105         NaN\n",
      "5           VW         Up!    1000     929  105         NaN\n",
      "6        Skoda       Fabia    1400    1109   90         NaN\n",
      "7     Mercedes     A-Class    1500    1365   92         NaN\n",
      "8         Ford      Fiesta    1500    1112   98         NaN\n",
      "9         Audi          A1    1600    1150   99         NaN\n",
      "10     Hyundai         I20    1100     980   99         NaN\n",
      "11      Suzuki       Swift    1300     990  101         NaN\n",
      "12        Ford      Fiesta    1000    1112   99         NaN\n",
      "13       Honda       Civic    1600    1252   94         NaN\n",
      "14      Hundai         I30    1600    1326   97         NaN\n",
      "15        Opel       Astra    1600    1330   97         NaN\n",
      "16         BMW           1    1600    1365   99         NaN\n",
      "17       Mazda           3    2200    1280  104         NaN\n",
      "18       Skoda       Rapid    1600    1119  104         NaN\n",
      "19        Ford       Focus    2000    1328  105         NaN\n",
      "20        Ford      Mondeo    1600    1584   94         NaN\n",
      "21        Opel    Insignia    2000    1428   99         NaN\n",
      "22    Mercedes     C-Class    2100    1365   99         NaN\n",
      "23       Skoda     Octavia    1600    1415   99         NaN\n",
      "24       Volvo         S60    2000    1415   99         NaN\n",
      "25    Mercedes         CLA    1500    1465  102         NaN\n",
      "26        Audi          A4    2000    1490  104         NaN\n",
      "27        Audi          A6    2000    1725  114         NaN\n",
      "28       Volvo         V70    1600    1523  109         NaN\n",
      "29         BMW           5    2000    1705  114         NaN\n",
      "30    Mercedes     E-Class    2100    1605  115         NaN\n",
      "31       Volvo        XC70    2000    1746  117         NaN\n",
      "32        Ford       B-Max    1600    1235  104         NaN\n",
      "33         BMW           2    1600    1390  108         NaN\n",
      "34        Opel      Zafira    1600    1405  109         NaN\n",
      "35    Mercedes         SLK    2500    1395  120         NaN\n"
     ]
    }
   ],
   "source": [
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     client_id loan_type  loan_amount  repaid  loan_id  loan_start  \\\n",
      "0        46109      home        13672       0    10243  2002-04-16   \n",
      "1        46109    credit         9794       0    10984  2003-10-21   \n",
      "2        46109      home        12734       1    10990  2006-02-01   \n",
      "3        46109      cash        12518       1    10596  2010-12-08   \n",
      "4        46109    credit        14049       1    11415  2010-07-07   \n",
      "..         ...       ...          ...     ...      ...         ...   \n",
      "438      26945     other        12963       0    10330  2001-11-26   \n",
      "439      26945    credit         1728       1    10248  2004-01-27   \n",
      "440      26945     other         9329       0    10154  2001-12-17   \n",
      "441      26945      home         4197       0    10333  2003-10-16   \n",
      "442      26945      home         3643       0    11434  2010-03-24   \n",
      "\n",
      "       loan_end  rate  \n",
      "0    2003-12-20  2.15  \n",
      "1    2005-07-17  1.25  \n",
      "2    2007-07-05  0.68  \n",
      "3    2013-05-05  1.24  \n",
      "4    2012-05-21  3.13  \n",
      "..          ...   ...  \n",
      "438  2004-06-11  2.46  \n",
      "439  2005-06-21  5.27  \n",
      "440  2004-07-22  5.65  \n",
      "441  2005-07-10  4.50  \n",
      "442  2011-12-22  0.13  \n",
      "\n",
      "[443 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleImputer()\n",
    "Replace missing values using a descriptive statistic (e.g. mean, median, or\n",
    "most frequent) along each column, or using a constant value.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Person   Age       City Bool  Marks\n",
      "0  Person2  24.0  Bangalore   No     47\n",
      "1  Person3  19.0      Delhi   No     89\n",
      "2  Person4   NaN     Mumbai  Yes     93\n",
      "3  Person5  24.0  Hyderabad   No     85\n",
      "4  Person1  17.0    Chennai  Yes     98\n",
      "\n",
      "\n",
      "[24. 19. 21. 24. 17.]\n"
     ]
    }
   ],
   "source": [
    "impute = SimpleImputer(missing_values=np.nan, strategy=\"mean\",fill_value='F')\n",
    "print(df); print('\\n')\n",
    "\n",
    "data = impute.fit_transform(df['Age'].values.reshape(-1,1))[:,0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_dummies(<coloumn>)\n",
    "\n",
    "Each variable is converted in as many 0/1 variables as there are different\n",
    "values. Columns in the output are each named after a value; if the input is\n",
    "a DataFrame, the name of the original variable is prepended to the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Model_1  Model_2  Model_3  Model_5  Model_500  Model_A-Class  Model_A1  Model_A4  Model_A6  Model_Astra  Model_Aygo  Model_B-Max  Model_C-Class  Model_CLA  Model_Citigo  Model_Civic  Model_Cooper  Model_E-Class  Model_Fabia  Model_Fiesta  Model_Focus  Model_I20  Model_I30  Model_Insignia  Model_Mondeo  Model_Octavia  Model_Rapid  Model_S60  Model_SLK  Model_Space Star  Model_Swift  Model_Up!  Model_V70  Model_XC70  Model_Zafira\n",
      "0     False    False    False    False      False          False     False     False     False        False        True        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "1     False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False              True        False      False      False       False         False\n",
      "2     False    False    False    False      False          False     False     False     False        False       False        False          False      False          True        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "3     False    False    False    False       True          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "4     False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False          True          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "5     False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False       True      False       False         False\n",
      "6     False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False         True         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "7     False    False    False    False      False           True     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "8     False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False          True        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "9     False    False    False    False      False          False      True     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "10    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False       True      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "11    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False         True      False      False       False         False\n",
      "12    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False          True        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "13    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False         True         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "14    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False       True           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "15    False    False    False    False      False          False     False     False     False         True       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "16     True    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "17    False    False     True    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "18    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False         True      False      False             False        False      False      False       False         False\n",
      "19    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False         True      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "20    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False          True          False        False      False      False             False        False      False      False       False         False\n",
      "21    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False            True         False          False        False      False      False             False        False      False      False       False         False\n",
      "22    False    False    False    False      False          False     False     False     False        False       False        False           True      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "23    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False           True        False      False      False             False        False      False      False       False         False\n",
      "24    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False       True      False             False        False      False      False       False         False\n",
      "25    False    False    False    False      False          False     False     False     False        False       False        False          False       True         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "26    False    False    False    False      False          False     False      True     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "27    False    False    False    False      False          False     False     False      True        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "28    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False       True       False         False\n",
      "29    False    False    False     True      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "30    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False           True        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "31    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False        True         False\n",
      "32    False    False    False    False      False          False     False     False     False        False       False         True          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "33    False     True    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "34    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False          True\n",
      "35    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False       True             False        False      False      False       False         False\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('datasets/cars.csv')                  #Reads cars.csv file\n",
    "\n",
    "new_cars = pd.get_dummies(df1[['Model']])\n",
    "print(new_cars.to_string())\n",
    "#print(new_cars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler()\n",
      "[[0.         0.         1.         0.38709677]\n",
      " [1.         0.58823529 0.         0.        ]\n",
      " [0.19047619 1.         0.35714286 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "data = [[12,21,45,32],[33,51,31,20],[16,72,36,51]]\n",
    "scaler = MinMaxScaler()\n",
    "print(scaler.fit(data))\n",
    "print(scaler.transform(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv('datasets/cars.csv')\n",
    "Vol_data = cars[['Volume']]\n",
    "Weight_data = cars[['Weight']]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "stdscaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Transform features by scaling each feature to a given range.\n",
      "\n",
      "This estimator scales and translates each feature individually such\n",
      "that it is in the given range on the training set, e.g. between\n",
      "zero and one.\n",
      "\n",
      "The transformation is given by::\n",
      "\n",
      "    X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
      "    X_scaled = X_std * (max - min) + min\n",
      "\n",
      "where min, max = feature_range.\n",
      "\n",
      "This transformation is often used as an alternative to zero mean,\n",
      "unit variance scaling.\n",
      "\n",
      "`MinMaxScaler` doesn't reduce the effect of outliers, but it linearly\n",
      "scales them down into a fixed range, where the largest occurring data point\n",
      "corresponds to the maximum value and the smallest one corresponds to the\n",
      "minimum value. For an example visualization, refer to :ref:`Compare\n",
      "MinMaxScaler with other scalers <plot_all_scaling_minmax_scaler_section>`.\n",
      "\n",
      "Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "feature_range : tuple (min, max), default=(0, 1)\n",
      "    Desired range of transformed data.\n",
      "\n",
      "copy : bool, default=True\n",
      "    Set to False to perform inplace row normalization and avoid a\n",
      "    copy (if the input is already a numpy array).\n",
      "\n",
      "clip : bool, default=False\n",
      "    Set to True to clip transformed values of held-out data to\n",
      "    provided `feature range`.\n",
      "\n",
      "    .. versionadded:: 0.24\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "min_ : ndarray of shape (n_features,)\n",
      "    Per feature adjustment for minimum. Equivalent to\n",
      "    ``min - X.min(axis=0) * self.scale_``\n",
      "\n",
      "scale_ : ndarray of shape (n_features,)\n",
      "    Per feature relative scaling of the data. Equivalent to\n",
      "    ``(max - min) / (X.max(axis=0) - X.min(axis=0))``\n",
      "\n",
      "    .. versionadded:: 0.17\n",
      "       *scale_* attribute.\n",
      "\n",
      "data_min_ : ndarray of shape (n_features,)\n",
      "    Per feature minimum seen in the data\n",
      "\n",
      "    .. versionadded:: 0.17\n",
      "       *data_min_*\n",
      "\n",
      "data_max_ : ndarray of shape (n_features,)\n",
      "    Per feature maximum seen in the data\n",
      "\n",
      "    .. versionadded:: 0.17\n",
      "       *data_max_*\n",
      "\n",
      "data_range_ : ndarray of shape (n_features,)\n",
      "    Per feature range ``(data_max_ - data_min_)`` seen in the data\n",
      "\n",
      "    .. versionadded:: 0.17\n",
      "       *data_range_*\n",
      "\n",
      "n_features_in_ : int\n",
      "    Number of features seen during :term:`fit`.\n",
      "\n",
      "    .. versionadded:: 0.24\n",
      "\n",
      "n_samples_seen_ : int\n",
      "    The number of samples processed by the estimator.\n",
      "    It will be reset on new calls to fit, but increments across\n",
      "    ``partial_fit`` calls.\n",
      "\n",
      "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "    Names of features seen during :term:`fit`. Defined only when `X`\n",
      "    has feature names that are all strings.\n",
      "\n",
      "    .. versionadded:: 1.0\n",
      "\n",
      "See Also\n",
      "--------\n",
      "minmax_scale : Equivalent function without the estimator API.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      "transform.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from sklearn.preprocessing import MinMaxScaler\n",
      ">>> data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
      ">>> scaler = MinMaxScaler()\n",
      ">>> print(scaler.fit(data))\n",
      "MinMaxScaler()\n",
      ">>> print(scaler.data_max_)\n",
      "[ 1. 18.]\n",
      ">>> print(scaler.transform(data))\n",
      "[[0.   0.  ]\n",
      " [0.25 0.25]\n",
      " [0.5  0.5 ]\n",
      " [1.   1.  ]]\n",
      ">>> print(scaler.transform([[2, 2]]))\n",
      "[[1.5 0. ]]\n",
      "\u001b[0;31mFile:\u001b[0m           ~/.local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "MinMaxScaler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_std\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Standardize features by removing the mean and scaling to unit variance.\n",
      "\n",
      "The standard score of a sample `x` is calculated as:\n",
      "\n",
      "    z = (x - u) / s\n",
      "\n",
      "where `u` is the mean of the training samples or zero if `with_mean=False`,\n",
      "and `s` is the standard deviation of the training samples or one if\n",
      "`with_std=False`.\n",
      "\n",
      "Centering and scaling happen independently on each feature by computing\n",
      "the relevant statistics on the samples in the training set. Mean and\n",
      "standard deviation are then stored to be used on later data using\n",
      ":meth:`transform`.\n",
      "\n",
      "Standardization of a dataset is a common requirement for many\n",
      "machine learning estimators: they might behave badly if the\n",
      "individual features do not more or less look like standard normally\n",
      "distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
      "\n",
      "For instance many elements used in the objective function of\n",
      "a learning algorithm (such as the RBF kernel of Support Vector\n",
      "Machines or the L1 and L2 regularizers of linear models) assume that\n",
      "all features are centered around 0 and have variance in the same\n",
      "order. If a feature has a variance that is orders of magnitude larger\n",
      "than others, it might dominate the objective function and make the\n",
      "estimator unable to learn from other features correctly as expected.\n",
      "\n",
      "`StandardScaler` is sensitive to outliers, and the features may scale\n",
      "differently from each other in the presence of outliers. For an example\n",
      "visualization, refer to :ref:`Compare StandardScaler with other scalers\n",
      "<plot_all_scaling_standard_scaler_section>`.\n",
      "\n",
      "This scaler can also be applied to sparse CSR or CSC matrices by passing\n",
      "`with_mean=False` to avoid breaking the sparsity structure of the data.\n",
      "\n",
      "Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "copy : bool, default=True\n",
      "    If False, try to avoid a copy and do inplace scaling instead.\n",
      "    This is not guaranteed to always work inplace; e.g. if the data is\n",
      "    not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n",
      "    returned.\n",
      "\n",
      "with_mean : bool, default=True\n",
      "    If True, center the data before scaling.\n",
      "    This does not work (and will raise an exception) when attempted on\n",
      "    sparse matrices, because centering them entails building a dense\n",
      "    matrix which in common use cases is likely to be too large to fit in\n",
      "    memory.\n",
      "\n",
      "with_std : bool, default=True\n",
      "    If True, scale the data to unit variance (or equivalently,\n",
      "    unit standard deviation).\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "scale_ : ndarray of shape (n_features,) or None\n",
      "    Per feature relative scaling of the data to achieve zero mean and unit\n",
      "    variance. Generally this is calculated using `np.sqrt(var_)`. If a\n",
      "    variance is zero, we can't achieve unit variance, and the data is left\n",
      "    as-is, giving a scaling factor of 1. `scale_` is equal to `None`\n",
      "    when `with_std=False`.\n",
      "\n",
      "    .. versionadded:: 0.17\n",
      "       *scale_*\n",
      "\n",
      "mean_ : ndarray of shape (n_features,) or None\n",
      "    The mean value for each feature in the training set.\n",
      "    Equal to ``None`` when ``with_mean=False`` and ``with_std=False``.\n",
      "\n",
      "var_ : ndarray of shape (n_features,) or None\n",
      "    The variance for each feature in the training set. Used to compute\n",
      "    `scale_`. Equal to ``None`` when ``with_mean=False`` and\n",
      "    ``with_std=False``.\n",
      "\n",
      "n_features_in_ : int\n",
      "    Number of features seen during :term:`fit`.\n",
      "\n",
      "    .. versionadded:: 0.24\n",
      "\n",
      "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "    Names of features seen during :term:`fit`. Defined only when `X`\n",
      "    has feature names that are all strings.\n",
      "\n",
      "    .. versionadded:: 1.0\n",
      "\n",
      "n_samples_seen_ : int or ndarray of shape (n_features,)\n",
      "    The number of samples processed by the estimator for each feature.\n",
      "    If there are no missing samples, the ``n_samples_seen`` will be an\n",
      "    integer, otherwise it will be an array of dtype int. If\n",
      "    `sample_weights` are used it will be a float (if no missing data)\n",
      "    or an array of dtype float that sums the weights seen so far.\n",
      "    Will be reset on new calls to fit, but increments across\n",
      "    ``partial_fit`` calls.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "scale : Equivalent function without the estimator API.\n",
      "\n",
      ":class:`~sklearn.decomposition.PCA` : Further removes the linear\n",
      "    correlation across features with 'whiten=True'.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      "transform.\n",
      "\n",
      "We use a biased estimator for the standard deviation, equivalent to\n",
      "`numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n",
      "affect model performance.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from sklearn.preprocessing import StandardScaler\n",
      ">>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
      ">>> scaler = StandardScaler()\n",
      ">>> print(scaler.fit(data))\n",
      "StandardScaler()\n",
      ">>> print(scaler.mean_)\n",
      "[0.5 0.5]\n",
      ">>> print(scaler.transform(data))\n",
      "[[-1. -1.]\n",
      " [-1. -1.]\n",
      " [ 1.  1.]\n",
      " [ 1.  1.]]\n",
      ">>> print(scaler.transform([[2, 2]]))\n",
      "[[3. 3.]]\n",
      "\u001b[0;31mFile:\u001b[0m           ~/.local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "StandardScaler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Fit to data, then transform it.\n",
      "\n",
      "Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "and returns a transformed version of `X`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "X : array-like of shape (n_samples, n_features)\n",
      "    Input samples.\n",
      "\n",
      "y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "    Target values (None for unsupervised transformations).\n",
      "\n",
      "**fit_params : dict\n",
      "    Additional fit parameters.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "    Transformed array.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.10/site-packages/sklearn/base.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "MinMaxScaler.fit_transform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Fit to data, then transform it.\n",
      "\n",
      "Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "and returns a transformed version of `X`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "X : array-like of shape (n_samples, n_features)\n",
      "    Input samples.\n",
      "\n",
      "y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "    Target values (None for unsupervised transformations).\n",
      "\n",
      "**fit_params : dict\n",
      "    Additional fit parameters.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "    Transformed array.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.10/site-packages/sklearn/base.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "StandardScaler.fit_transform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0625]\n",
      " [0.1875]\n",
      " [0.0625]\n",
      " [0.    ]\n",
      " [0.375 ]\n",
      " [0.0625]\n",
      " [0.3125]\n",
      " [0.375 ]\n",
      " [0.375 ]\n",
      " [0.4375]\n",
      " [0.125 ]\n",
      " [0.25  ]\n",
      " [0.0625]\n",
      " [0.4375]\n",
      " [0.4375]\n",
      " [0.4375]\n",
      " [0.4375]\n",
      " [0.8125]\n",
      " [0.4375]\n",
      " [0.6875]\n",
      " [0.4375]\n",
      " [0.6875]\n",
      " [0.75  ]\n",
      " [0.4375]\n",
      " [0.6875]\n",
      " [0.375 ]\n",
      " [0.6875]\n",
      " [0.6875]\n",
      " [0.4375]\n",
      " [0.6875]\n",
      " [0.75  ]\n",
      " [0.6875]\n",
      " [0.4375]\n",
      " [0.4375]\n",
      " [0.4375]\n",
      " [1.    ]]\n",
      "\n",
      "\n",
      "[[0.        ]\n",
      " [0.38702929]\n",
      " [0.14539749]\n",
      " [0.07845188]\n",
      " [0.36610879]\n",
      " [0.14539749]\n",
      " [0.33368201]\n",
      " [0.60146444]\n",
      " [0.33682008]\n",
      " [0.37656904]\n",
      " [0.19874477]\n",
      " [0.20920502]\n",
      " [0.33682008]\n",
      " [0.4832636 ]\n",
      " [0.56066946]\n",
      " [0.56485356]\n",
      " [0.60146444]\n",
      " [0.5125523 ]\n",
      " [0.34414226]\n",
      " [0.56276151]\n",
      " [0.83054393]\n",
      " [0.66736402]\n",
      " [0.60146444]\n",
      " [0.65376569]\n",
      " [0.65376569]\n",
      " [0.70606695]\n",
      " [0.73221757]\n",
      " [0.97803347]\n",
      " [0.7667364 ]\n",
      " [0.95711297]\n",
      " [0.85251046]\n",
      " [1.        ]\n",
      " [0.46548117]\n",
      " [0.62761506]\n",
      " [0.64330544]\n",
      " [0.63284519]]\n",
      "\n",
      "\n",
      "[[-1.59336644]\n",
      " [-1.07190106]\n",
      " [-1.59336644]\n",
      " [-1.85409913]\n",
      " [-0.28970299]\n",
      " [-1.59336644]\n",
      " [-0.55043568]\n",
      " [-0.28970299]\n",
      " [-0.28970299]\n",
      " [-0.0289703 ]\n",
      " [-1.33263375]\n",
      " [-0.81116837]\n",
      " [-1.59336644]\n",
      " [-0.0289703 ]\n",
      " [-0.0289703 ]\n",
      " [-0.0289703 ]\n",
      " [-0.0289703 ]\n",
      " [ 1.53542584]\n",
      " [-0.0289703 ]\n",
      " [ 1.01396046]\n",
      " [-0.0289703 ]\n",
      " [ 1.01396046]\n",
      " [ 1.27469315]\n",
      " [-0.0289703 ]\n",
      " [ 1.01396046]\n",
      " [-0.28970299]\n",
      " [ 1.01396046]\n",
      " [ 1.01396046]\n",
      " [-0.0289703 ]\n",
      " [ 1.01396046]\n",
      " [ 1.27469315]\n",
      " [ 1.01396046]\n",
      " [-0.0289703 ]\n",
      " [-0.0289703 ]\n",
      " [-0.0289703 ]\n",
      " [ 2.31762392]]\n",
      "\n",
      "\n",
      "[[-2.10389253]\n",
      " [-0.55407235]\n",
      " [-1.52166278]\n",
      " [-1.78973979]\n",
      " [-0.63784641]\n",
      " [-1.52166278]\n",
      " [-0.76769621]\n",
      " [ 0.3046118 ]\n",
      " [-0.7551301 ]\n",
      " [-0.59595938]\n",
      " [-1.30803892]\n",
      " [-1.26615189]\n",
      " [-0.7551301 ]\n",
      " [-0.16871166]\n",
      " [ 0.14125238]\n",
      " [ 0.15800719]\n",
      " [ 0.3046118 ]\n",
      " [-0.05142797]\n",
      " [-0.72580918]\n",
      " [ 0.14962979]\n",
      " [ 1.2219378 ]\n",
      " [ 0.5685001 ]\n",
      " [ 0.3046118 ]\n",
      " [ 0.51404696]\n",
      " [ 0.51404696]\n",
      " [ 0.72348212]\n",
      " [ 0.8281997 ]\n",
      " [ 1.81254495]\n",
      " [ 0.96642691]\n",
      " [ 1.72877089]\n",
      " [ 1.30990057]\n",
      " [ 1.90050772]\n",
      " [-0.23991961]\n",
      " [ 0.40932938]\n",
      " [ 0.47215993]\n",
      " [ 0.4302729 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scaler.fit(Vol_data)\n",
    "mm_data_vol = scaler.transform(Vol_data)\n",
    "print(mm_data_vol)\n",
    "print('\\n')\n",
    "\n",
    "scaler.fit(Weight_data)\n",
    "mm_data_weight = scaler.transform(Weight_data)\n",
    "print(mm_data_weight)\n",
    "print('\\n')\n",
    "\n",
    "stdscaler.fit(Vol_data)\n",
    "stddata_vol = stdscaler.transform(Vol_data)\n",
    "print(stddata_vol)\n",
    "print('\\n')\n",
    "\n",
    "stdscaler.fit(Weight_data)\n",
    "stddata_weight = stdscaler.transform(Weight_data)\n",
    "print(stddata_weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
