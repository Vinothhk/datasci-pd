{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinoth/.local/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/vinoth/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n",
      "/tmp/ipykernel_84313/61355013.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Person   Age       City Bool  Marks\n",
      "0  Person2  24.0  Bangalore   No     47\n",
      "1  Person3  19.0      Delhi   No     89\n",
      "2  Person4   NaN     Mumbai  Yes     93\n",
      "3  Person5  24.0  Hyderabad   No     85\n",
      "4  Person1  17.0    Chennai  Yes     98\n"
     ]
    }
   ],
   "source": [
    "#Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "\n",
    "df  = pd.read_csv('../datasets/dataset1.csv')     # Read the file\n",
    "print(df)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person    0\n",
       "Age       1\n",
       "City      0\n",
       "Bool      0\n",
       "Marks     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    24.0\n",
      "1    19.0\n",
      "2     NaN\n",
      "3    24.0\n",
      "4    17.0\n",
      "Name: Age, dtype: float64\n",
      "\n",
      "\n",
      "             Age      Marks\n",
      "count   4.000000   5.000000\n",
      "mean   21.000000  82.400000\n",
      "std     3.559026  20.366639\n",
      "min    17.000000  47.000000\n",
      "25%    18.500000  85.000000\n",
      "50%    21.500000  89.000000\n",
      "75%    24.000000  93.000000\n",
      "max    24.000000  98.000000\n",
      "\n",
      "\n",
      "(5, 5)\n",
      "\n",
      "\n",
      "   Person    Age   City   Bool  Marks\n",
      "0   False  False  False  False  False\n",
      "1   False  False  False  False  False\n",
      "2   False   True  False  False  False\n",
      "3   False  False  False  False  False\n",
      "4   False  False  False  False  False\n"
     ]
    }
   ],
   "source": [
    "print(df['Age'])    # Access Coloumn\n",
    "print('\\n')\n",
    "\n",
    "print(df.describe())  # Generate descriptive statistics\n",
    "print('\\n')\n",
    "\n",
    "print(df.shape)                     #representing the dimensionality of the DataFrame\n",
    "print('\\n')\n",
    "\n",
    "print(df.isnull())                  #Return a boolean same-sized object indicating if the values are NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Person   Age       City Bool  Marks\n",
      "0  Person2  24.0  Bangalore   No     47\n",
      "1  Person3  19.0      Delhi   No     89\n",
      "2  Person4   NaN     Mumbai  Yes     93\n",
      "3  Person5  24.0  Hyderabad   No     85\n",
      "4  Person1  17.0    Chennai  Yes     98\n",
      "\n",
      "\n",
      "    Person   Age       City Bool  Marks\n",
      "0  Person2  24.0  Bangalore   No     47\n",
      "1  Person3  19.0      Delhi   No     89\n",
      "2  Person4   NaN     Mumbai  Yes     93\n",
      "\n",
      "\n",
      "    Person   Age       City Bool  Marks\n",
      "0  Person2  24.0  Bangalore   No     47\n",
      "\n",
      "\n",
      "19.0\n"
     ]
    }
   ],
   "source": [
    "print(df); print('\\n')\n",
    "\n",
    "print(df.head(3))                       # Returns first n rows\n",
    "print('\\n')\n",
    "\n",
    "print(df.loc[df['City']=='Bangalore'])  # Access a group of rows and columns by label(s) \n",
    "print('\\n')\n",
    "\n",
    "print(df.iloc[1,1])                     # Purely integer-location based indexing for selection by position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.groupby - Group DataFrame using a mapper or by a Series of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Person   Age                     City  Marks\n",
      "Bool                                                             \n",
      "No    Person2Person3Person5  67.0  BangaloreDelhiHyderabad    221\n",
      "Yes          Person4Person1  17.0            MumbaiChennai    191\n",
      "\n",
      "\n",
      "Bool\n",
      "No     22.333333\n",
      "Yes    17.000000\n",
      "Name: Age, dtype: float64\n",
      "\n",
      "\n",
      "    Person   Age       City Bool  Marks\n",
      "0  Person2  24.0  Bangalore   No     47\n",
      "1  Person3  19.0      Delhi   No     89\n",
      "3  Person5  24.0  Hyderabad   No     85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_84313/4171253851.py:5: FutureWarning: The provided callable <function mean at 0x7e09841bc5e0> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  print(x['Age'].agg(np.mean))\n",
      "/tmp/ipykernel_84313/4171253851.py:8: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n",
      "  print(x.get_group('No'))         # Print Rows of a specific group\n"
     ]
    }
   ],
   "source": [
    "x = df.groupby(['Bool'])         # Group DataFrame using a mapper or by a Series of columns.\n",
    "print(x.sum())\n",
    "print('\\n')\n",
    "\n",
    "print(x['Age'].agg(np.mean))\n",
    "print('\\n')\n",
    "\n",
    "print(x.get_group('No'))         # Print Rows of a specific group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### describe()\n",
    "\n",
    "Generate descriptive statistics that include those that summarize the central\n",
    "tendency, dispersion and shape of a\n",
    "dataset's distribution, excluding ``NaN`` values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2 = pd.read_csv('../datasets/loans.csv')                 #Reads loans.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf1\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(1, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:182\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (1, 2)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf2\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3809\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3805\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3806\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3807\u001b[0m     ):\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: (1, 2)"
     ]
    }
   ],
   "source": [
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleImputer()\n",
    "Replace missing values using a descriptive statistic (e.g. mean, median, or\n",
    "most frequent) along each column, or using a constant value.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Person   Age       City Bool  Marks\n",
      "0  Person2  24.0  Bangalore   No     47\n",
      "1  Person3  19.0      Delhi   No     89\n",
      "2  Person4   NaN     Mumbai  Yes     93\n",
      "3  Person5  24.0  Hyderabad   No     85\n",
      "4  Person1  17.0    Chennai  Yes     98\n",
      "\n",
      "\n",
      "[24. 19. 21. 24. 17.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impute = SimpleImputer(missing_values=np.nan, strategy=\"mean\",fill_value='F')\n",
    "print(df); print('\\n')\n",
    "\n",
    "data = impute.fit_transform(df['Age'].values.reshape(-1,1))[:,0]\n",
    "print(data)\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_dummies(<coloumn>)\n",
    "\n",
    "Each variable is converted in as many 0/1 variables as there are different\n",
    "values. Columns in the output are each named after a value; if the input is\n",
    "a DataFrame, the name of the original variable is prepended to the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/cars.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatasets/cars.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m                  \u001b[38;5;66;03m#Reads cars.csv file\u001b[39;00m\n\u001b[1;32m      3\u001b[0m new_cars \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(df1[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(new_cars\u001b[38;5;241m.\u001b[39mto_string())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1024\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1011\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1012\u001b[0m     dialect,\n\u001b[1;32m   1013\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[1;32m   1022\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1024\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:618\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    615\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 618\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1618\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1617\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1618\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1878\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1877\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1878\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1889\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/cars.csv'"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('datasets/cars.csv')                  #Reads cars.csv file\n",
    "\n",
    "new_cars = pd.get_dummies(df1[['Model']])\n",
    "print(new_cars.to_string())\n",
    "#print(new_cars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler()\n",
      "[[0.         0.         1.         0.38709677]\n",
      " [1.         0.58823529 0.         0.        ]\n",
      " [0.19047619 1.         0.35714286 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "data = [[12,21,45,32],[33,51,31,20],[16,72,36,51]]\n",
    "scaler = MinMaxScaler()\n",
    "print(scaler.fit(data))\n",
    "print(scaler.transform(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv('datasets/cars.csv')\n",
    "Vol_data = cars[['Volume']]\n",
    "Weight_data = cars[['Weight']]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "stdscaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Transform features by scaling each feature to a given range.\n",
      "\n",
      "This estimator scales and translates each feature individually such\n",
      "that it is in the given range on the training set, e.g. between\n",
      "zero and one.\n",
      "\n",
      "The transformation is given by::\n",
      "\n",
      "    X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
      "    X_scaled = X_std * (max - min) + min\n",
      "\n",
      "where min, max = feature_range.\n",
      "\n",
      "This transformation is often used as an alternative to zero mean,\n",
      "unit variance scaling.\n",
      "\n",
      "`MinMaxScaler` doesn't reduce the effect of outliers, but it linearly\n",
      "scales them down into a fixed range, where the largest occurring data point\n",
      "corresponds to the maximum value and the smallest one corresponds to the\n",
      "minimum value. For an example visualization, refer to :ref:`Compare\n",
      "MinMaxScaler with other scalers <plot_all_scaling_minmax_scaler_section>`.\n",
      "\n",
      "Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "feature_range : tuple (min, max), default=(0, 1)\n",
      "    Desired range of transformed data.\n",
      "\n",
      "copy : bool, default=True\n",
      "    Set to False to perform inplace row normalization and avoid a\n",
      "    copy (if the input is already a numpy array).\n",
      "\n",
      "clip : bool, default=False\n",
      "    Set to True to clip transformed values of held-out data to\n",
      "    provided `feature range`.\n",
      "\n",
      "    .. versionadded:: 0.24\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "min_ : ndarray of shape (n_features,)\n",
      "    Per feature adjustment for minimum. Equivalent to\n",
      "    ``min - X.min(axis=0) * self.scale_``\n",
      "\n",
      "scale_ : ndarray of shape (n_features,)\n",
      "    Per feature relative scaling of the data. Equivalent to\n",
      "    ``(max - min) / (X.max(axis=0) - X.min(axis=0))``\n",
      "\n",
      "    .. versionadded:: 0.17\n",
      "       *scale_* attribute.\n",
      "\n",
      "data_min_ : ndarray of shape (n_features,)\n",
      "    Per feature minimum seen in the data\n",
      "\n",
      "    .. versionadded:: 0.17\n",
      "       *data_min_*\n",
      "\n",
      "data_max_ : ndarray of shape (n_features,)\n",
      "    Per feature maximum seen in the data\n",
      "\n",
      "    .. versionadded:: 0.17\n",
      "       *data_max_*\n",
      "\n",
      "data_range_ : ndarray of shape (n_features,)\n",
      "    Per feature range ``(data_max_ - data_min_)`` seen in the data\n",
      "\n",
      "    .. versionadded:: 0.17\n",
      "       *data_range_*\n",
      "\n",
      "n_features_in_ : int\n",
      "    Number of features seen during :term:`fit`.\n",
      "\n",
      "    .. versionadded:: 0.24\n",
      "\n",
      "n_samples_seen_ : int\n",
      "    The number of samples processed by the estimator.\n",
      "    It will be reset on new calls to fit, but increments across\n",
      "    ``partial_fit`` calls.\n",
      "\n",
      "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "    Names of features seen during :term:`fit`. Defined only when `X`\n",
      "    has feature names that are all strings.\n",
      "\n",
      "    .. versionadded:: 1.0\n",
      "\n",
      "See Also\n",
      "--------\n",
      "minmax_scale : Equivalent function without the estimator API.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      "transform.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from sklearn.preprocessing import MinMaxScaler\n",
      ">>> data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
      ">>> scaler = MinMaxScaler()\n",
      ">>> print(scaler.fit(data))\n",
      "MinMaxScaler()\n",
      ">>> print(scaler.data_max_)\n",
      "[ 1. 18.]\n",
      ">>> print(scaler.transform(data))\n",
      "[[0.   0.  ]\n",
      " [0.25 0.25]\n",
      " [0.5  0.5 ]\n",
      " [1.   1.  ]]\n",
      ">>> print(scaler.transform([[2, 2]]))\n",
      "[[1.5 0. ]]\n",
      "\u001b[0;31mFile:\u001b[0m           ~/.local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "MinMaxScaler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_std\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Standardize features by removing the mean and scaling to unit variance.\n",
      "\n",
      "The standard score of a sample `x` is calculated as:\n",
      "\n",
      "    z = (x - u) / s\n",
      "\n",
      "where `u` is the mean of the training samples or zero if `with_mean=False`,\n",
      "and `s` is the standard deviation of the training samples or one if\n",
      "`with_std=False`.\n",
      "\n",
      "Centering and scaling happen independently on each feature by computing\n",
      "the relevant statistics on the samples in the training set. Mean and\n",
      "standard deviation are then stored to be used on later data using\n",
      ":meth:`transform`.\n",
      "\n",
      "Standardization of a dataset is a common requirement for many\n",
      "machine learning estimators: they might behave badly if the\n",
      "individual features do not more or less look like standard normally\n",
      "distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
      "\n",
      "For instance many elements used in the objective function of\n",
      "a learning algorithm (such as the RBF kernel of Support Vector\n",
      "Machines or the L1 and L2 regularizers of linear models) assume that\n",
      "all features are centered around 0 and have variance in the same\n",
      "order. If a feature has a variance that is orders of magnitude larger\n",
      "than others, it might dominate the objective function and make the\n",
      "estimator unable to learn from other features correctly as expected.\n",
      "\n",
      "`StandardScaler` is sensitive to outliers, and the features may scale\n",
      "differently from each other in the presence of outliers. For an example\n",
      "visualization, refer to :ref:`Compare StandardScaler with other scalers\n",
      "<plot_all_scaling_standard_scaler_section>`.\n",
      "\n",
      "This scaler can also be applied to sparse CSR or CSC matrices by passing\n",
      "`with_mean=False` to avoid breaking the sparsity structure of the data.\n",
      "\n",
      "Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "copy : bool, default=True\n",
      "    If False, try to avoid a copy and do inplace scaling instead.\n",
      "    This is not guaranteed to always work inplace; e.g. if the data is\n",
      "    not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n",
      "    returned.\n",
      "\n",
      "with_mean : bool, default=True\n",
      "    If True, center the data before scaling.\n",
      "    This does not work (and will raise an exception) when attempted on\n",
      "    sparse matrices, because centering them entails building a dense\n",
      "    matrix which in common use cases is likely to be too large to fit in\n",
      "    memory.\n",
      "\n",
      "with_std : bool, default=True\n",
      "    If True, scale the data to unit variance (or equivalently,\n",
      "    unit standard deviation).\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "scale_ : ndarray of shape (n_features,) or None\n",
      "    Per feature relative scaling of the data to achieve zero mean and unit\n",
      "    variance. Generally this is calculated using `np.sqrt(var_)`. If a\n",
      "    variance is zero, we can't achieve unit variance, and the data is left\n",
      "    as-is, giving a scaling factor of 1. `scale_` is equal to `None`\n",
      "    when `with_std=False`.\n",
      "\n",
      "    .. versionadded:: 0.17\n",
      "       *scale_*\n",
      "\n",
      "mean_ : ndarray of shape (n_features,) or None\n",
      "    The mean value for each feature in the training set.\n",
      "    Equal to ``None`` when ``with_mean=False`` and ``with_std=False``.\n",
      "\n",
      "var_ : ndarray of shape (n_features,) or None\n",
      "    The variance for each feature in the training set. Used to compute\n",
      "    `scale_`. Equal to ``None`` when ``with_mean=False`` and\n",
      "    ``with_std=False``.\n",
      "\n",
      "n_features_in_ : int\n",
      "    Number of features seen during :term:`fit`.\n",
      "\n",
      "    .. versionadded:: 0.24\n",
      "\n",
      "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "    Names of features seen during :term:`fit`. Defined only when `X`\n",
      "    has feature names that are all strings.\n",
      "\n",
      "    .. versionadded:: 1.0\n",
      "\n",
      "n_samples_seen_ : int or ndarray of shape (n_features,)\n",
      "    The number of samples processed by the estimator for each feature.\n",
      "    If there are no missing samples, the ``n_samples_seen`` will be an\n",
      "    integer, otherwise it will be an array of dtype int. If\n",
      "    `sample_weights` are used it will be a float (if no missing data)\n",
      "    or an array of dtype float that sums the weights seen so far.\n",
      "    Will be reset on new calls to fit, but increments across\n",
      "    ``partial_fit`` calls.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "scale : Equivalent function without the estimator API.\n",
      "\n",
      ":class:`~sklearn.decomposition.PCA` : Further removes the linear\n",
      "    correlation across features with 'whiten=True'.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      "transform.\n",
      "\n",
      "We use a biased estimator for the standard deviation, equivalent to\n",
      "`numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n",
      "affect model performance.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from sklearn.preprocessing import StandardScaler\n",
      ">>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
      ">>> scaler = StandardScaler()\n",
      ">>> print(scaler.fit(data))\n",
      "StandardScaler()\n",
      ">>> print(scaler.mean_)\n",
      "[0.5 0.5]\n",
      ">>> print(scaler.transform(data))\n",
      "[[-1. -1.]\n",
      " [-1. -1.]\n",
      " [ 1.  1.]\n",
      " [ 1.  1.]]\n",
      ">>> print(scaler.transform([[2, 2]]))\n",
      "[[3. 3.]]\n",
      "\u001b[0;31mFile:\u001b[0m           ~/.local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "StandardScaler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Fit to data, then transform it.\n",
      "\n",
      "Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "and returns a transformed version of `X`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "X : array-like of shape (n_samples, n_features)\n",
      "    Input samples.\n",
      "\n",
      "y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "    Target values (None for unsupervised transformations).\n",
      "\n",
      "**fit_params : dict\n",
      "    Additional fit parameters.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "    Transformed array.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.10/site-packages/sklearn/base.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "MinMaxScaler.fit_transform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Fit to data, then transform it.\n",
      "\n",
      "Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "and returns a transformed version of `X`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "X : array-like of shape (n_samples, n_features)\n",
      "    Input samples.\n",
      "\n",
      "y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "    Target values (None for unsupervised transformations).\n",
      "\n",
      "**fit_params : dict\n",
      "    Additional fit parameters.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "    Transformed array.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.10/site-packages/sklearn/base.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "StandardScaler.fit_transform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0625]\n",
      " [0.1875]\n",
      " [0.0625]\n",
      " [0.    ]\n",
      " [0.375 ]\n",
      " [0.0625]\n",
      " [0.3125]\n",
      " [0.375 ]\n",
      " [0.375 ]\n",
      " [0.4375]\n",
      " [0.125 ]\n",
      " [0.25  ]\n",
      " [0.0625]\n",
      " [0.4375]\n",
      " [0.4375]\n",
      " [0.4375]\n",
      " [0.4375]\n",
      " [0.8125]\n",
      " [0.4375]\n",
      " [0.6875]\n",
      " [0.4375]\n",
      " [0.6875]\n",
      " [0.75  ]\n",
      " [0.4375]\n",
      " [0.6875]\n",
      " [0.375 ]\n",
      " [0.6875]\n",
      " [0.6875]\n",
      " [0.4375]\n",
      " [0.6875]\n",
      " [0.75  ]\n",
      " [0.6875]\n",
      " [0.4375]\n",
      " [0.4375]\n",
      " [0.4375]\n",
      " [1.    ]]\n",
      "\n",
      "\n",
      "[[0.        ]\n",
      " [0.38702929]\n",
      " [0.14539749]\n",
      " [0.07845188]\n",
      " [0.36610879]\n",
      " [0.14539749]\n",
      " [0.33368201]\n",
      " [0.60146444]\n",
      " [0.33682008]\n",
      " [0.37656904]\n",
      " [0.19874477]\n",
      " [0.20920502]\n",
      " [0.33682008]\n",
      " [0.4832636 ]\n",
      " [0.56066946]\n",
      " [0.56485356]\n",
      " [0.60146444]\n",
      " [0.5125523 ]\n",
      " [0.34414226]\n",
      " [0.56276151]\n",
      " [0.83054393]\n",
      " [0.66736402]\n",
      " [0.60146444]\n",
      " [0.65376569]\n",
      " [0.65376569]\n",
      " [0.70606695]\n",
      " [0.73221757]\n",
      " [0.97803347]\n",
      " [0.7667364 ]\n",
      " [0.95711297]\n",
      " [0.85251046]\n",
      " [1.        ]\n",
      " [0.46548117]\n",
      " [0.62761506]\n",
      " [0.64330544]\n",
      " [0.63284519]]\n",
      "\n",
      "\n",
      "[[-1.59336644]\n",
      " [-1.07190106]\n",
      " [-1.59336644]\n",
      " [-1.85409913]\n",
      " [-0.28970299]\n",
      " [-1.59336644]\n",
      " [-0.55043568]\n",
      " [-0.28970299]\n",
      " [-0.28970299]\n",
      " [-0.0289703 ]\n",
      " [-1.33263375]\n",
      " [-0.81116837]\n",
      " [-1.59336644]\n",
      " [-0.0289703 ]\n",
      " [-0.0289703 ]\n",
      " [-0.0289703 ]\n",
      " [-0.0289703 ]\n",
      " [ 1.53542584]\n",
      " [-0.0289703 ]\n",
      " [ 1.01396046]\n",
      " [-0.0289703 ]\n",
      " [ 1.01396046]\n",
      " [ 1.27469315]\n",
      " [-0.0289703 ]\n",
      " [ 1.01396046]\n",
      " [-0.28970299]\n",
      " [ 1.01396046]\n",
      " [ 1.01396046]\n",
      " [-0.0289703 ]\n",
      " [ 1.01396046]\n",
      " [ 1.27469315]\n",
      " [ 1.01396046]\n",
      " [-0.0289703 ]\n",
      " [-0.0289703 ]\n",
      " [-0.0289703 ]\n",
      " [ 2.31762392]]\n",
      "\n",
      "\n",
      "[[-2.10389253]\n",
      " [-0.55407235]\n",
      " [-1.52166278]\n",
      " [-1.78973979]\n",
      " [-0.63784641]\n",
      " [-1.52166278]\n",
      " [-0.76769621]\n",
      " [ 0.3046118 ]\n",
      " [-0.7551301 ]\n",
      " [-0.59595938]\n",
      " [-1.30803892]\n",
      " [-1.26615189]\n",
      " [-0.7551301 ]\n",
      " [-0.16871166]\n",
      " [ 0.14125238]\n",
      " [ 0.15800719]\n",
      " [ 0.3046118 ]\n",
      " [-0.05142797]\n",
      " [-0.72580918]\n",
      " [ 0.14962979]\n",
      " [ 1.2219378 ]\n",
      " [ 0.5685001 ]\n",
      " [ 0.3046118 ]\n",
      " [ 0.51404696]\n",
      " [ 0.51404696]\n",
      " [ 0.72348212]\n",
      " [ 0.8281997 ]\n",
      " [ 1.81254495]\n",
      " [ 0.96642691]\n",
      " [ 1.72877089]\n",
      " [ 1.30990057]\n",
      " [ 1.90050772]\n",
      " [-0.23991961]\n",
      " [ 0.40932938]\n",
      " [ 0.47215993]\n",
      " [ 0.4302729 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scaler.fit(Vol_data)\n",
    "mm_data_vol = scaler.transform(Vol_data)\n",
    "print(mm_data_vol)\n",
    "print('\\n')\n",
    "\n",
    "scaler.fit(Weight_data)\n",
    "mm_data_weight = scaler.transform(Weight_data)\n",
    "print(mm_data_weight)\n",
    "print('\\n')\n",
    "\n",
    "stdscaler.fit(Vol_data)\n",
    "stddata_vol = stdscaler.transform(Vol_data)\n",
    "print(stddata_vol)\n",
    "print('\\n')\n",
    "\n",
    "stdscaler.fit(Weight_data)\n",
    "stddata_weight = stdscaler.transform(Weight_data)\n",
    "print(stddata_weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
