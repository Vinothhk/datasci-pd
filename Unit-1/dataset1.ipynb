{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Person   Age       City Bool  Marks\n",
      "0  Person2  24.0  Bangalore   No     47\n",
      "1  Person3  19.0      Delhi   No     89\n",
      "2  Person4   NaN     Mumbai  Yes     93\n",
      "3  Person5  24.0  Hyderabad   No     85\n",
      "4  Person1  17.0    Chennai  Yes     98\n"
     ]
    }
   ],
   "source": [
    "#Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "\n",
    "df  = pd.read_csv('../datasets/dataset1.csv')     # Read the file\n",
    "print(df)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    24.0\n",
      "1    19.0\n",
      "2     NaN\n",
      "3    24.0\n",
      "4    17.0\n",
      "Name: Age, dtype: float64\n",
      "\n",
      "\n",
      "             Age      Marks\n",
      "count   4.000000   5.000000\n",
      "mean   21.000000  82.400000\n",
      "std     3.559026  20.366639\n",
      "min    17.000000  47.000000\n",
      "25%    18.500000  85.000000\n",
      "50%    21.500000  89.000000\n",
      "75%    24.000000  93.000000\n",
      "max    24.000000  98.000000\n",
      "\n",
      "\n",
      "(5, 5)\n",
      "\n",
      "\n",
      "   Person    Age   City   Bool  Marks\n",
      "0   False  False  False  False  False\n",
      "1   False  False  False  False  False\n",
      "2   False   True  False  False  False\n",
      "3   False  False  False  False  False\n",
      "4   False  False  False  False  False\n"
     ]
    }
   ],
   "source": [
    "print(df['Age'])    # Access Coloumn\n",
    "print('\\n')\n",
    "\n",
    "print(df.describe())  # Generate descriptive statistics\n",
    "print('\\n')\n",
    "\n",
    "print(df.shape)                     #representing the dimensionality of the DataFrame\n",
    "print('\\n')\n",
    "\n",
    "print(df.isnull())                  #Return a boolean same-sized object indicating if the values are NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Person   Age       City Bool  Marks\n",
      "0  Person2  24.0  Bangalore   No     47\n",
      "1  Person3  19.0      Delhi   No     89\n",
      "2  Person4   NaN     Mumbai  Yes     93\n",
      "3  Person5  24.0  Hyderabad   No     85\n",
      "4  Person1  17.0    Chennai  Yes     98\n",
      "\n",
      "\n",
      "    Person   Age       City Bool  Marks\n",
      "0  Person2  24.0  Bangalore   No     47\n",
      "1  Person3  19.0      Delhi   No     89\n",
      "2  Person4   NaN     Mumbai  Yes     93\n",
      "\n",
      "\n",
      "    Person   Age       City Bool  Marks\n",
      "0  Person2  24.0  Bangalore   No     47\n",
      "\n",
      "\n",
      "19.0\n"
     ]
    }
   ],
   "source": [
    "print(df); print('\\n')\n",
    "\n",
    "print(df.head(3))                       # Returns first n rows\n",
    "print('\\n')\n",
    "\n",
    "print(df.loc[df['City']=='Bangalore'])  # Access a group of rows and columns by label(s) \n",
    "print('\\n')\n",
    "\n",
    "print(df.iloc[1,1])                     # Purely integer-location based indexing for selection by position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.groupby - Group DataFrame using a mapper or by a Series of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Bool                 Person   Age                     City  Marks\n",
      "0   No  Person2Person3Person5  67.0  BangaloreDelhiHyderabad    221\n",
      "1  Yes         Person4Person1  17.0            MumbaiChennai    191\n",
      "\n",
      "\n",
      "Bool\n",
      "No     22.333333\n",
      "Yes    17.000000\n",
      "Name: Age, dtype: float64\n",
      "\n",
      "\n",
      "    Person   Age       City Bool  Marks\n",
      "0  Person2  24.0  Bangalore   No     47\n",
      "1  Person3  19.0      Delhi   No     89\n",
      "3  Person5  24.0  Hyderabad   No     85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_146761/2869077663.py:5: FutureWarning: The provided callable <function mean at 0x73acd14e6200> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  print(x['Age'].agg(np.mean))\n",
      "/tmp/ipykernel_146761/2869077663.py:8: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n",
      "  print(x.get_group('No'))         # Print Rows of a specific group\n"
     ]
    }
   ],
   "source": [
    "x = df.groupby(['Bool'])         # Group DataFrame using a mapper or by a Series of columns.\n",
    "print(x.sum().reset_index())\n",
    "print('\\n')\n",
    "\n",
    "print(x['Age'].agg(np.mean))\n",
    "print('\\n')\n",
    "\n",
    "print(x.get_group('No'))         # Print Rows of a specific group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### describe()\n",
    "\n",
    "Generate descriptive statistics that include those that summarize the central\n",
    "tendency, dispersion and shape of a\n",
    "dataset's distribution, excluding ``NaN`` values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2 = pd.read_csv('../datasets/loans.csv')                 #Reads loans.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf1\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(1, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:182\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (1, 2)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf2\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3809\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3805\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3806\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3807\u001b[0m     ):\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: (1, 2)"
     ]
    }
   ],
   "source": [
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleImputer()\n",
    "Replace missing values using a descriptive statistic (e.g. mean, median, or\n",
    "most frequent) along each column, or using a constant value.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Person   Age       City Bool  Marks\n",
      "0  Person2  24.0  Bangalore   No     47\n",
      "1  Person3  19.0      Delhi   No     89\n",
      "2  Person4   NaN     Mumbai  Yes     93\n",
      "3  Person5  24.0  Hyderabad   No     85\n",
      "4  Person1  17.0    Chennai  Yes     98\n",
      "\n",
      "\n",
      "[24. 19. 21. 24. 17.]\n"
     ]
    }
   ],
   "source": [
    "impute = SimpleImputer(missing_values=np.nan, strategy=\"mean\",fill_value='F')\n",
    "print(df); print('\\n')\n",
    "\n",
    "data = impute.fit_transform(df['Age'].values.reshape(-1,1))[:,0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_dummies(<coloumn>)\n",
    "\n",
    "Each variable is converted in as many 0/1 variables as there are different\n",
    "values. Columns in the output are each named after a value; if the input is\n",
    "a DataFrame, the name of the original variable is prepended to the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Model_1  Model_2  Model_3  Model_5  Model_500  Model_A-Class  Model_A1  Model_A4  Model_A6  Model_Astra  Model_Aygo  Model_B-Max  Model_C-Class  Model_CLA  Model_Citigo  Model_Civic  Model_Cooper  Model_E-Class  Model_Fabia  Model_Fiesta  Model_Focus  Model_I20  Model_I30  Model_Insignia  Model_Mondeo  Model_Octavia  Model_Rapid  Model_S60  Model_SLK  Model_Space Star  Model_Swift  Model_Up!  Model_V70  Model_XC70  Model_Zafira\n",
      "0     False    False    False    False      False          False     False     False     False        False        True        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "1     False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False              True        False      False      False       False         False\n",
      "2     False    False    False    False      False          False     False     False     False        False       False        False          False      False          True        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "3     False    False    False    False       True          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "4     False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False          True          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "5     False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False       True      False       False         False\n",
      "6     False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False         True         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "7     False    False    False    False      False           True     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "8     False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False          True        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "9     False    False    False    False      False          False      True     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "10    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False       True      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "11    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False         True      False      False       False         False\n",
      "12    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False          True        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "13    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False         True         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "14    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False       True           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "15    False    False    False    False      False          False     False     False     False         True       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "16     True    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "17    False    False     True    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "18    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False         True      False      False             False        False      False      False       False         False\n",
      "19    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False         True      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "20    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False          True          False        False      False      False             False        False      False      False       False         False\n",
      "21    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False            True         False          False        False      False      False             False        False      False      False       False         False\n",
      "22    False    False    False    False      False          False     False     False     False        False       False        False           True      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "23    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False           True        False      False      False             False        False      False      False       False         False\n",
      "24    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False       True      False             False        False      False      False       False         False\n",
      "25    False    False    False    False      False          False     False     False     False        False       False        False          False       True         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "26    False    False    False    False      False          False     False      True     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "27    False    False    False    False      False          False     False     False      True        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "28    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False       True       False         False\n",
      "29    False    False    False     True      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "30    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False           True        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "31    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False        True         False\n",
      "32    False    False    False    False      False          False     False     False     False        False       False         True          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "33    False     True    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False         False\n",
      "34    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False      False             False        False      False      False       False          True\n",
      "35    False    False    False    False      False          False     False     False     False        False       False        False          False      False         False        False         False          False        False         False        False      False      False           False         False          False        False      False       True             False        False      False      False       False         False\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('datasets/cars.csv')                  #Reads cars.csv file\n",
    "\n",
    "new_cars = pd.get_dummies(df1[['Model']])\n",
    "print(new_cars.to_string())\n",
    "#print(new_cars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler()\n",
      "[[0.         0.         1.         0.38709677]\n",
      " [1.         0.58823529 0.         0.        ]\n",
      " [0.19047619 1.         0.35714286 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "data = [[12,21,45,32],[33,51,31,20],[16,72,36,51]]\n",
    "scaler = MinMaxScaler()\n",
    "print(scaler.fit(data))\n",
    "print(scaler.transform(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv('datasets/cars.csv')\n",
    "Vol_data = cars[['Volume']]\n",
    "Weight_data = cars[['Weight']]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "stdscaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Transform features by scaling each feature to a given range.\n",
      "\n",
      "This estimator scales and translates each feature individually such\n",
      "that it is in the given range on the training set, e.g. between\n",
      "zero and one.\n",
      "\n",
      "The transformation is given by::\n",
      "\n",
      "    X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
      "    X_scaled = X_std * (max - min) + min\n",
      "\n",
      "where min, max = feature_range.\n",
      "\n",
      "This transformation is often used as an alternative to zero mean,\n",
      "unit variance scaling.\n",
      "\n",
      "`MinMaxScaler` doesn't reduce the effect of outliers, but it linearly\n",
      "scales them down into a fixed range, where the largest occurring data point\n",
      "corresponds to the maximum value and the smallest one corresponds to the\n",
      "minimum value. For an example visualization, refer to :ref:`Compare\n",
      "MinMaxScaler with other scalers <plot_all_scaling_minmax_scaler_section>`.\n",
      "\n",
      "Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "feature_range : tuple (min, max), default=(0, 1)\n",
      "    Desired range of transformed data.\n",
      "\n",
      "copy : bool, default=True\n",
      "    Set to False to perform inplace row normalization and avoid a\n",
      "    copy (if the input is already a numpy array).\n",
      "\n",
      "clip : bool, default=False\n",
      "    Set to True to clip transformed values of held-out data to\n",
      "    provided `feature range`.\n",
      "\n",
      "    .. versionadded:: 0.24\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "min_ : ndarray of shape (n_features,)\n",
      "    Per feature adjustment for minimum. Equivalent to\n",
      "    ``min - X.min(axis=0) * self.scale_``\n",
      "\n",
      "scale_ : ndarray of shape (n_features,)\n",
      "    Per feature relative scaling of the data. Equivalent to\n",
      "    ``(max - min) / (X.max(axis=0) - X.min(axis=0))``\n",
      "\n",
      "    .. versionadded:: 0.17\n",
      "       *scale_* attribute.\n",
      "\n",
      "data_min_ : ndarray of shape (n_features,)\n",
      "    Per feature minimum seen in the data\n",
      "\n",
      "    .. versionadded:: 0.17\n",
      "       *data_min_*\n",
      "\n",
      "data_max_ : ndarray of shape (n_features,)\n",
      "    Per feature maximum seen in the data\n",
      "\n",
      "    .. versionadded:: 0.17\n",
      "       *data_max_*\n",
      "\n",
      "data_range_ : ndarray of shape (n_features,)\n",
      "    Per feature range ``(data_max_ - data_min_)`` seen in the data\n",
      "\n",
      "    .. versionadded:: 0.17\n",
      "       *data_range_*\n",
      "\n",
      "n_features_in_ : int\n",
      "    Number of features seen during :term:`fit`.\n",
      "\n",
      "    .. versionadded:: 0.24\n",
      "\n",
      "n_samples_seen_ : int\n",
      "    The number of samples processed by the estimator.\n",
      "    It will be reset on new calls to fit, but increments across\n",
      "    ``partial_fit`` calls.\n",
      "\n",
      "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "    Names of features seen during :term:`fit`. Defined only when `X`\n",
      "    has feature names that are all strings.\n",
      "\n",
      "    .. versionadded:: 1.0\n",
      "\n",
      "See Also\n",
      "--------\n",
      "minmax_scale : Equivalent function without the estimator API.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      "transform.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from sklearn.preprocessing import MinMaxScaler\n",
      ">>> data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
      ">>> scaler = MinMaxScaler()\n",
      ">>> print(scaler.fit(data))\n",
      "MinMaxScaler()\n",
      ">>> print(scaler.data_max_)\n",
      "[ 1. 18.]\n",
      ">>> print(scaler.transform(data))\n",
      "[[0.   0.  ]\n",
      " [0.25 0.25]\n",
      " [0.5  0.5 ]\n",
      " [1.   1.  ]]\n",
      ">>> print(scaler.transform([[2, 2]]))\n",
      "[[1.5 0. ]]\n",
      "\u001b[0;31mFile:\u001b[0m           ~/.local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "MinMaxScaler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_std\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Standardize features by removing the mean and scaling to unit variance.\n",
      "\n",
      "The standard score of a sample `x` is calculated as:\n",
      "\n",
      "    z = (x - u) / s\n",
      "\n",
      "where `u` is the mean of the training samples or zero if `with_mean=False`,\n",
      "and `s` is the standard deviation of the training samples or one if\n",
      "`with_std=False`.\n",
      "\n",
      "Centering and scaling happen independently on each feature by computing\n",
      "the relevant statistics on the samples in the training set. Mean and\n",
      "standard deviation are then stored to be used on later data using\n",
      ":meth:`transform`.\n",
      "\n",
      "Standardization of a dataset is a common requirement for many\n",
      "machine learning estimators: they might behave badly if the\n",
      "individual features do not more or less look like standard normally\n",
      "distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
      "\n",
      "For instance many elements used in the objective function of\n",
      "a learning algorithm (such as the RBF kernel of Support Vector\n",
      "Machines or the L1 and L2 regularizers of linear models) assume that\n",
      "all features are centered around 0 and have variance in the same\n",
      "order. If a feature has a variance that is orders of magnitude larger\n",
      "than others, it might dominate the objective function and make the\n",
      "estimator unable to learn from other features correctly as expected.\n",
      "\n",
      "`StandardScaler` is sensitive to outliers, and the features may scale\n",
      "differently from each other in the presence of outliers. For an example\n",
      "visualization, refer to :ref:`Compare StandardScaler with other scalers\n",
      "<plot_all_scaling_standard_scaler_section>`.\n",
      "\n",
      "This scaler can also be applied to sparse CSR or CSC matrices by passing\n",
      "`with_mean=False` to avoid breaking the sparsity structure of the data.\n",
      "\n",
      "Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "copy : bool, default=True\n",
      "    If False, try to avoid a copy and do inplace scaling instead.\n",
      "    This is not guaranteed to always work inplace; e.g. if the data is\n",
      "    not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n",
      "    returned.\n",
      "\n",
      "with_mean : bool, default=True\n",
      "    If True, center the data before scaling.\n",
      "    This does not work (and will raise an exception) when attempted on\n",
      "    sparse matrices, because centering them entails building a dense\n",
      "    matrix which in common use cases is likely to be too large to fit in\n",
      "    memory.\n",
      "\n",
      "with_std : bool, default=True\n",
      "    If True, scale the data to unit variance (or equivalently,\n",
      "    unit standard deviation).\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "scale_ : ndarray of shape (n_features,) or None\n",
      "    Per feature relative scaling of the data to achieve zero mean and unit\n",
      "    variance. Generally this is calculated using `np.sqrt(var_)`. If a\n",
      "    variance is zero, we can't achieve unit variance, and the data is left\n",
      "    as-is, giving a scaling factor of 1. `scale_` is equal to `None`\n",
      "    when `with_std=False`.\n",
      "\n",
      "    .. versionadded:: 0.17\n",
      "       *scale_*\n",
      "\n",
      "mean_ : ndarray of shape (n_features,) or None\n",
      "    The mean value for each feature in the training set.\n",
      "    Equal to ``None`` when ``with_mean=False`` and ``with_std=False``.\n",
      "\n",
      "var_ : ndarray of shape (n_features,) or None\n",
      "    The variance for each feature in the training set. Used to compute\n",
      "    `scale_`. Equal to ``None`` when ``with_mean=False`` and\n",
      "    ``with_std=False``.\n",
      "\n",
      "n_features_in_ : int\n",
      "    Number of features seen during :term:`fit`.\n",
      "\n",
      "    .. versionadded:: 0.24\n",
      "\n",
      "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "    Names of features seen during :term:`fit`. Defined only when `X`\n",
      "    has feature names that are all strings.\n",
      "\n",
      "    .. versionadded:: 1.0\n",
      "\n",
      "n_samples_seen_ : int or ndarray of shape (n_features,)\n",
      "    The number of samples processed by the estimator for each feature.\n",
      "    If there are no missing samples, the ``n_samples_seen`` will be an\n",
      "    integer, otherwise it will be an array of dtype int. If\n",
      "    `sample_weights` are used it will be a float (if no missing data)\n",
      "    or an array of dtype float that sums the weights seen so far.\n",
      "    Will be reset on new calls to fit, but increments across\n",
      "    ``partial_fit`` calls.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "scale : Equivalent function without the estimator API.\n",
      "\n",
      ":class:`~sklearn.decomposition.PCA` : Further removes the linear\n",
      "    correlation across features with 'whiten=True'.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      "transform.\n",
      "\n",
      "We use a biased estimator for the standard deviation, equivalent to\n",
      "`numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n",
      "affect model performance.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from sklearn.preprocessing import StandardScaler\n",
      ">>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
      ">>> scaler = StandardScaler()\n",
      ">>> print(scaler.fit(data))\n",
      "StandardScaler()\n",
      ">>> print(scaler.mean_)\n",
      "[0.5 0.5]\n",
      ">>> print(scaler.transform(data))\n",
      "[[-1. -1.]\n",
      " [-1. -1.]\n",
      " [ 1.  1.]\n",
      " [ 1.  1.]]\n",
      ">>> print(scaler.transform([[2, 2]]))\n",
      "[[3. 3.]]\n",
      "\u001b[0;31mFile:\u001b[0m           ~/.local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "StandardScaler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Fit to data, then transform it.\n",
      "\n",
      "Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "and returns a transformed version of `X`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "X : array-like of shape (n_samples, n_features)\n",
      "    Input samples.\n",
      "\n",
      "y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "    Target values (None for unsupervised transformations).\n",
      "\n",
      "**fit_params : dict\n",
      "    Additional fit parameters.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "    Transformed array.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.10/site-packages/sklearn/base.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "MinMaxScaler.fit_transform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Fit to data, then transform it.\n",
      "\n",
      "Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "and returns a transformed version of `X`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "X : array-like of shape (n_samples, n_features)\n",
      "    Input samples.\n",
      "\n",
      "y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "    Target values (None for unsupervised transformations).\n",
      "\n",
      "**fit_params : dict\n",
      "    Additional fit parameters.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "    Transformed array.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.10/site-packages/sklearn/base.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "StandardScaler.fit_transform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0625]\n",
      " [0.1875]\n",
      " [0.0625]\n",
      " [0.    ]\n",
      " [0.375 ]\n",
      " [0.0625]\n",
      " [0.3125]\n",
      " [0.375 ]\n",
      " [0.375 ]\n",
      " [0.4375]\n",
      " [0.125 ]\n",
      " [0.25  ]\n",
      " [0.0625]\n",
      " [0.4375]\n",
      " [0.4375]\n",
      " [0.4375]\n",
      " [0.4375]\n",
      " [0.8125]\n",
      " [0.4375]\n",
      " [0.6875]\n",
      " [0.4375]\n",
      " [0.6875]\n",
      " [0.75  ]\n",
      " [0.4375]\n",
      " [0.6875]\n",
      " [0.375 ]\n",
      " [0.6875]\n",
      " [0.6875]\n",
      " [0.4375]\n",
      " [0.6875]\n",
      " [0.75  ]\n",
      " [0.6875]\n",
      " [0.4375]\n",
      " [0.4375]\n",
      " [0.4375]\n",
      " [1.    ]]\n",
      "\n",
      "\n",
      "[[0.        ]\n",
      " [0.38702929]\n",
      " [0.14539749]\n",
      " [0.07845188]\n",
      " [0.36610879]\n",
      " [0.14539749]\n",
      " [0.33368201]\n",
      " [0.60146444]\n",
      " [0.33682008]\n",
      " [0.37656904]\n",
      " [0.19874477]\n",
      " [0.20920502]\n",
      " [0.33682008]\n",
      " [0.4832636 ]\n",
      " [0.56066946]\n",
      " [0.56485356]\n",
      " [0.60146444]\n",
      " [0.5125523 ]\n",
      " [0.34414226]\n",
      " [0.56276151]\n",
      " [0.83054393]\n",
      " [0.66736402]\n",
      " [0.60146444]\n",
      " [0.65376569]\n",
      " [0.65376569]\n",
      " [0.70606695]\n",
      " [0.73221757]\n",
      " [0.97803347]\n",
      " [0.7667364 ]\n",
      " [0.95711297]\n",
      " [0.85251046]\n",
      " [1.        ]\n",
      " [0.46548117]\n",
      " [0.62761506]\n",
      " [0.64330544]\n",
      " [0.63284519]]\n",
      "\n",
      "\n",
      "[[-1.59336644]\n",
      " [-1.07190106]\n",
      " [-1.59336644]\n",
      " [-1.85409913]\n",
      " [-0.28970299]\n",
      " [-1.59336644]\n",
      " [-0.55043568]\n",
      " [-0.28970299]\n",
      " [-0.28970299]\n",
      " [-0.0289703 ]\n",
      " [-1.33263375]\n",
      " [-0.81116837]\n",
      " [-1.59336644]\n",
      " [-0.0289703 ]\n",
      " [-0.0289703 ]\n",
      " [-0.0289703 ]\n",
      " [-0.0289703 ]\n",
      " [ 1.53542584]\n",
      " [-0.0289703 ]\n",
      " [ 1.01396046]\n",
      " [-0.0289703 ]\n",
      " [ 1.01396046]\n",
      " [ 1.27469315]\n",
      " [-0.0289703 ]\n",
      " [ 1.01396046]\n",
      " [-0.28970299]\n",
      " [ 1.01396046]\n",
      " [ 1.01396046]\n",
      " [-0.0289703 ]\n",
      " [ 1.01396046]\n",
      " [ 1.27469315]\n",
      " [ 1.01396046]\n",
      " [-0.0289703 ]\n",
      " [-0.0289703 ]\n",
      " [-0.0289703 ]\n",
      " [ 2.31762392]]\n",
      "\n",
      "\n",
      "[[-2.10389253]\n",
      " [-0.55407235]\n",
      " [-1.52166278]\n",
      " [-1.78973979]\n",
      " [-0.63784641]\n",
      " [-1.52166278]\n",
      " [-0.76769621]\n",
      " [ 0.3046118 ]\n",
      " [-0.7551301 ]\n",
      " [-0.59595938]\n",
      " [-1.30803892]\n",
      " [-1.26615189]\n",
      " [-0.7551301 ]\n",
      " [-0.16871166]\n",
      " [ 0.14125238]\n",
      " [ 0.15800719]\n",
      " [ 0.3046118 ]\n",
      " [-0.05142797]\n",
      " [-0.72580918]\n",
      " [ 0.14962979]\n",
      " [ 1.2219378 ]\n",
      " [ 0.5685001 ]\n",
      " [ 0.3046118 ]\n",
      " [ 0.51404696]\n",
      " [ 0.51404696]\n",
      " [ 0.72348212]\n",
      " [ 0.8281997 ]\n",
      " [ 1.81254495]\n",
      " [ 0.96642691]\n",
      " [ 1.72877089]\n",
      " [ 1.30990057]\n",
      " [ 1.90050772]\n",
      " [-0.23991961]\n",
      " [ 0.40932938]\n",
      " [ 0.47215993]\n",
      " [ 0.4302729 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scaler.fit(Vol_data)\n",
    "mm_data_vol = scaler.transform(Vol_data)\n",
    "print(mm_data_vol)\n",
    "print('\\n')\n",
    "\n",
    "scaler.fit(Weight_data)\n",
    "mm_data_weight = scaler.transform(Weight_data)\n",
    "print(mm_data_weight)\n",
    "print('\\n')\n",
    "\n",
    "stdscaler.fit(Vol_data)\n",
    "stddata_vol = stdscaler.transform(Vol_data)\n",
    "print(stddata_vol)\n",
    "print('\\n')\n",
    "\n",
    "stdscaler.fit(Weight_data)\n",
    "stddata_weight = stdscaler.transform(Weight_data)\n",
    "print(stddata_weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
